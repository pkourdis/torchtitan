x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:28:43,832 - root - INFO - Starting job: Llama 3 70B training
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:28:43,833 - root - INFO - [GC] Initial GC collection. 0.00 seconds.
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:28:47,450 - root - INFO - Building 1-D device mesh with ['dp_shard'], [192]
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: 2025:03:18-13:28:47:(197414) |CCL_WARN| value of CCL_KVS_MODE changed to be mpi (default:pmi)
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: 2025:03:18-13:28:47:(197414) |CCL_WARN| value of CCL_KVS_CONNECTION_TIMEOUT changed to be 140 (default:120)
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: 2025:03:18-13:28:47:(197414) |CCL_WARN| value of CCL_PROCESS_LAUNCHER changed to be pmix (default:hydra)
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:29:12,071 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:29:12,071 - root - INFO - Preparing c4 dataset from /flare/Aurora_deployment/sgoswami/datavol/mldata/hf/c4
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:29:52,460 - root - INFO - Building llama3 70B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, norm_type='rmsnorm')
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:29:54,121 - root - INFO - XPU capacity: Intel(R) Data Center GPU Max 1550 with 64.00GiB memory
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:29:54,745 - root - WARNING - Peak flops undefined for: Intel(R) Data Center GPU Max 1550, fallback to A100
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:29:54,748 - root - INFO - [34mModel llama3 70B [31msize: 70,553,706,496 total parameters[39m
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:29:54,749 - root - INFO - Applied full activation checkpointing to the model
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:30:03,553 - root - INFO - Applied FSDP to the model
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:30:08,476 - root - WARNING - Peak flops undefined for: Intel(R) Data Center GPU Max 1550, fallback to A100
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:30:08,476 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:30:08,476 - root - INFO - XPU memory usage for model: 1.44GiB(2.26%)
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:30:08,479 - root - INFO - Trainer initialized.
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:30:08,479 - root - INFO - Training starts at step 1, with local batch size 8, global batch size 1536, sequence length 1024, total steps 1000 (warmup 200)
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:33:02,066 - root - INFO - [31mstep:  1  [32mloss: 12.2489  [33mmemory: 50.22GiB(78.47%)  [34mtps: 44  [36mtflops: 18.59  [35mmfu: 5.96%[39m
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:33:02,067 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:38:31,679 - root - INFO - [31mstep:  5  [32mloss: 10.9066  [33mmemory: 51.83GiB(80.98%)  [34mtps: 99  [36mtflops: 42.26  [35mmfu: 13.54%[39m
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:45:22,348 - root - INFO - [31mstep: 10  [32mloss: 11.1400  [33mmemory: 51.83GiB(80.98%)  [34mtps: 100  [36mtflops: 42.40  [35mmfu: 13.59%[39m
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:52:14,187 - root - INFO - [31mstep: 15  [32mloss:  9.4310  [33mmemory: 51.83GiB(80.98%)  [34mtps: 99  [36mtflops: 42.28  [35mmfu: 13.55%[39m
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 13:59:06,266 - root - INFO - [31mstep: 20  [32mloss:  8.9120  [33mmemory: 51.83GiB(80.98%)  [34mtps: 99  [36mtflops: 42.25  [35mmfu: 13.54%[39m
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 14:05:58,299 - root - INFO - [31mstep: 25  [32mloss:  8.2764  [33mmemory: 51.83GiB(80.98%)  [34mtps: 99  [36mtflops: 42.26  [35mmfu: 13.54%[39m
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 14:12:49,474 - root - INFO - [31mstep: 30  [32mloss:  7.8876  [33mmemory: 51.83GiB(80.98%)  [34mtps: 100  [36mtflops: 42.34  [35mmfu: 13.57%[39m
x4711c4s3b0n0.hostmgmt2711.cm.aurora.alcf.anl.gov 0: [titan] 2025-03-18 14:19:40,613 - root - INFO - [31mstep: 35  [32mloss:  7.4984  [33mmemory: 51.83GiB(80.98%)  [34mtps: 100  [36mtflops: 42.35  [35mmfu: 13.57%[39m
